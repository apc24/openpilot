# -*- coding: utf-8 -*-
"""
ONNXå¤‰æ›å¾Œã®ãƒ¢ãƒ‡ãƒ«ä½¿ç”¨ä¾‹

ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯å¤‰æ›ã•ã‚ŒãŸONNXãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦
æ¨è«–ã‚’å®Ÿè¡Œã™ã‚‹æ–¹æ³•ã‚’ç¤ºã—ã¾ã™ã€‚
"""

import numpy as np
import time
from pathlib import Path
import sys
from typing import Dict

try:
    import onnxruntime as ort
except ImportError:
    print("ã‚¨ãƒ©ãƒ¼: onnxruntimeãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")
    print("ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„:")
    print("pip install onnxruntime")
    sys.exit(1)


class ONNXAutonomousDrivingModel:
    """
    ONNXå½¢å¼ã®è‡ªå‹•é‹è»¢ãƒ¢ãƒ‡ãƒ«ã®ãƒ©ãƒƒãƒ‘ãƒ¼ã‚¯ãƒ©ã‚¹
    
    å¤‰æ›ã•ã‚ŒãŸONNXãƒ¢ãƒ‡ãƒ«ã‚’ç°¡å˜ã«ä½¿ç”¨ã™ã‚‹ãŸã‚ã®ã‚¯ãƒ©ã‚¹ã§ã™ã€‚
    """
    
    def __init__(self, model_path: str, providers: list = None):
        """
        ONNXãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–
        
        Args:
            model_path (str): ONNXãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
            providers (list): ä½¿ç”¨ã™ã‚‹ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ï¼ˆå®Ÿè¡Œç’°å¢ƒï¼‰ã®ãƒªã‚¹ãƒˆ
        """
        self.model_path = model_path
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®è¨­å®š
        if providers is None:
            providers = ['CPUExecutionProvider']
            # GPUåˆ©ç”¨å¯èƒ½ã§ã‚ã‚Œã°è¿½åŠ 
            if ort.get_device() == 'GPU':
                providers.insert(0, 'CUDAExecutionProvider')
        
        print(f"ONNXãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­: {model_path}")
        print(f"ä½¿ç”¨ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {providers}")
        
        try:
            # ONNXãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä½œæˆ
            self.session = ort.InferenceSession(
                model_path, 
                providers=providers
            )
            
            # å…¥åŠ›ãƒ»å‡ºåŠ›æƒ…å ±ã‚’å–å¾—
            self.input_names = [input.name for input in self.session.get_inputs()]
            self.output_names = [output.name for output in self.session.get_outputs()]
            
            print(f"âœ“ ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿å®Œäº†")
            print(f"å…¥åŠ›: {self.input_names}")
            print(f"å‡ºåŠ›: {self.output_names}")
            
        except Exception as e:
            print(f"ã‚¨ãƒ©ãƒ¼: ONNXãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}")
            raise
    
    def predict(self, input_data: Dict[str, np.ndarray]) -> np.ndarray:
        """
        æ¨è«–ã‚’å®Ÿè¡Œ
        
        Args:
            input_data (Dict[str, np.ndarray]): å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®è¾æ›¸
            
        Returns:
            np.ndarray: äºˆæ¸¬çµæœï¼ˆåˆ¶å¾¡ä¿¡å·ï¼‰
        """
        try:
            # æ¨è«–å®Ÿè¡Œ
            outputs = self.session.run(self.output_names, input_data)
            return outputs[0]  # åˆ¶å¾¡å‡ºåŠ›ã‚’è¿”ã™
            
        except Exception as e:
            print(f"ã‚¨ãƒ©ãƒ¼: æ¨è«–å®Ÿè¡Œä¸­ã«å•é¡ŒãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
            raise
    
    def get_input_shapes(self) -> Dict[str, tuple]:
        """å…¥åŠ›ã®å½¢çŠ¶æƒ…å ±ã‚’å–å¾—"""
        shapes = {}
        for input_info in self.session.get_inputs():
            shapes[input_info.name] = input_info.shape
        return shapes
    
    def get_output_shapes(self) -> Dict[str, tuple]:
        """å‡ºåŠ›ã®å½¢çŠ¶æƒ…å ±ã‚’å–å¾—"""
        shapes = {}
        for output_info in self.session.get_outputs():
            shapes[output_info.name] = output_info.shape
        return shapes


def create_sample_input_data(batch_size: int = 1) -> Dict[str, np.ndarray]:
    """
    ã‚µãƒ³ãƒ—ãƒ«å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
    
    Args:
        batch_size (int): ãƒãƒƒãƒã‚µã‚¤ã‚º
        
    Returns:
        Dict[str, np.ndarray]: ã‚µãƒ³ãƒ—ãƒ«å…¥åŠ›ãƒ‡ãƒ¼ã‚¿
    """
    # å®Ÿéš›ã®ä½¿ç”¨æ™‚ã¯ã€ã“ã‚Œã‚‰ã‚’ã‚»ãƒ³ã‚µãƒ¼ã‹ã‚‰ã®å®Ÿãƒ‡ãƒ¼ã‚¿ã«ç½®ãæ›ãˆã‚‹
    sample_data = {
        # è»Šä¸¡çŠ¶æ…‹ï¼ˆé€Ÿåº¦ã€åŠ é€Ÿåº¦ã€ãƒ¨ãƒ¼è§’ãªã©8æ¬¡å…ƒï¼‰
        'carState': np.random.randn(batch_size, 8).astype(np.float32),
        # ãƒ¡ã‚¤ãƒ³ã‚«ãƒ¡ãƒ©ç”»åƒï¼ˆ3ãƒãƒ£ãƒ³ãƒãƒ«ã€224x224ï¼‰
        'mainCamera': np.random.randn(batch_size, 3, 224, 224).astype(np.float32),

        # ã‚ºãƒ¼ãƒ ã‚«ãƒ¡ãƒ©ç”»åƒï¼ˆ3ãƒãƒ£ãƒ³ãƒãƒ«ã€224x224ï¼‰
        'zoomCamera': np.random.randn(batch_size, 3, 224, 224).astype(np.float32),

        # ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³æƒ…å ±ï¼ˆ150æ¬¡å…ƒãƒ™ã‚¯ãƒˆãƒ«ï¼‰
        'navVector': np.random.randn(batch_size, 150).astype(np.float32)
    }
    
    return sample_data


def benchmark_performance(model: ONNXAutonomousDrivingModel, num_runs: int = 100):
    """
    ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
    
    Args:
        model (ONNXAutonomousDrivingModel): ãƒ†ã‚¹ãƒˆå¯¾è±¡ã®ãƒ¢ãƒ‡ãƒ«
        num_runs (int): å®Ÿè¡Œå›æ•°
    """
    print(f"\n=== ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ ({num_runs}å›å®Ÿè¡Œ) ===")
    
    # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ï¼ˆæœ€åˆã®æ•°å›ã¯é™¤å¤–ï¼‰
    warmup_data = create_sample_input_data(1)
    for _ in range(5):
        model.predict(warmup_data)
    
    # å®Ÿéš›ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
    times = []
    test_data = create_sample_input_data(1)
    
    for i in range(num_runs):
        start_time = time.time()
        result = model.predict(test_data)
        end_time = time.time()
        
        inference_time = (end_time - start_time) * 1000  # ãƒŸãƒªç§’
        times.append(inference_time)
        
        if i % 20 == 0:
            print(f"å®Ÿè¡Œ {i+1}/{num_runs}: {inference_time:.2f}ms")
    
    # çµ±è¨ˆæƒ…å ±ã®è¡¨ç¤º
    times = np.array(times)
    print(f"\nğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆ:")
    print(f"å¹³å‡æ¨è«–æ™‚é–“: {np.mean(times):.2f}ms")
    print(f"æœ€å°æ¨è«–æ™‚é–“: {np.min(times):.2f}ms")
    print(f"æœ€å¤§æ¨è«–æ™‚é–“: {np.max(times):.2f}ms")
    print(f"æ¨™æº–åå·®: {np.std(times):.2f}ms")
    print(f"FPS (ç†è«–å€¤): {1000/np.mean(times):.1f}")


def compare_batch_sizes(model: ONNXAutonomousDrivingModel):
    """
    ç•°ãªã‚‹ãƒãƒƒãƒã‚µã‚¤ã‚ºã§ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒ
    
    Args:
        model (ONNXAutonomousDrivingModel): ãƒ†ã‚¹ãƒˆå¯¾è±¡ã®ãƒ¢ãƒ‡ãƒ«
    """
    print(f"\n=== ãƒãƒƒãƒã‚µã‚¤ã‚ºåˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒ ===")
    
    batch_sizes = [1, 2, 4, 8]
    
    for batch_size in batch_sizes:
        print(f"\nãƒãƒƒãƒã‚µã‚¤ã‚º {batch_size}:")
        
        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æº–å‚™
        test_data = create_sample_input_data(batch_size)
        
        # è¤‡æ•°å›å®Ÿè¡Œã—ã¦å¹³å‡æ™‚é–“ã‚’æ¸¬å®š
        times = []
        for _ in range(20):
            start_time = time.time()
            result = model.predict(test_data)
            end_time = time.time()
            times.append((end_time - start_time) * 1000)
        
        avg_time = np.mean(times)
        per_sample_time = avg_time / batch_size
        
        print(f"  ãƒãƒƒãƒå…¨ä½“: {avg_time:.2f}ms")
        print(f"  ã‚µãƒ³ãƒ—ãƒ«å½“ãŸã‚Š: {per_sample_time:.2f}ms")
        print(f"  ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {1000/per_sample_time:.1f} samples/sec")


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("=== ONNXè‡ªå‹•é‹è»¢ãƒ¢ãƒ‡ãƒ« ä½¿ç”¨ä¾‹ ===\n")
    
    # ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’è¨­å®š
    # ã“ã®éƒ¨åˆ†ã‚’å®Ÿéš›ã®ONNXãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã«å¤‰æ›´ã—ã¦ãã ã•ã„
    model_path = "checkpoint_epoch_5_best.onnx"
    
    # ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª
    if not Path(model_path).exists():
        print(f"ã‚¨ãƒ©ãƒ¼: ONNXãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {model_path}")
        print("\nåˆ©ç”¨å¯èƒ½ãªONNXãƒ•ã‚¡ã‚¤ãƒ«:")
        
        models_dir = Path("models")
        if models_dir.exists():
            onnx_files = list(models_dir.glob("*.onnx"))
            if onnx_files:
                for onnx_file in onnx_files:
                    print(f"  - {onnx_file}")
                print(f"\nä¸Šè¨˜ã®ã„ãšã‚Œã‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã«å¤‰æ›´ã—ã¦ãã ã•ã„ã€‚")
            else:
                print("  è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                print("\næœ€åˆã«PyTorchãƒ¢ãƒ‡ãƒ«ã‚’ONNXå½¢å¼ã«å¤‰æ›ã—ã¦ãã ã•ã„:")
                print("python src/tools/convert_to_onnx.py models/checkpoint_epoch_5_best.pt")
        
        return
    
    try:
        # ONNXãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–
        model = ONNXAutonomousDrivingModel(model_path)
        
        # ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã®è¡¨ç¤º
        print(f"\nğŸ“‹ ãƒ¢ãƒ‡ãƒ«æƒ…å ±:")
        print(f"å…¥åŠ›å½¢çŠ¶: {model.get_input_shapes()}")
        print(f"å‡ºåŠ›å½¢çŠ¶: {model.get_output_shapes()}")
        
        # å˜ç™ºæ¨è«–ã®ãƒ†ã‚¹ãƒˆ
        print(f"\nğŸ”® å˜ç™ºæ¨è«–ãƒ†ã‚¹ãƒˆ:")
        sample_input = create_sample_input_data(1)
        
        start_time = time.time()
        result = model.predict(sample_input)
        inference_time = (time.time() - start_time) * 1000
        
        print(f"å…¥åŠ›ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶:")
        for name, data in sample_input.items():
            print(f"  {name}: {data.shape}")
        
        print(f"\nå‡ºåŠ›çµæœ:")
        print(f"  åˆ¶å¾¡ä¿¡å·: {result.shape}")
        print(f"  å€¤: {result[0]}")  # æœ€åˆã®ã‚µãƒ³ãƒ—ãƒ«ã®åˆ¶å¾¡å€¤
        print(f"  æ¨è«–æ™‚é–“: {inference_time:.2f}ms")
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ
        benchmark_performance(model)
        
        # ãƒãƒƒãƒã‚µã‚¤ã‚ºæ¯”è¼ƒ
        compare_batch_sizes(model)
        
        print(f"\nğŸ‰ ã™ã¹ã¦ã®ãƒ†ã‚¹ãƒˆãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸï¼")
        
    except Exception as e:
        print(f"ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return


if __name__ == '__main__':
    main()